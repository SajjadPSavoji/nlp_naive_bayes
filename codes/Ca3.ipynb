{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI CA3 \n",
    "sajjad pakdaman<br>\n",
    "810195517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Through this assignment both models suggested in discription was implemented and analysed.<br>\n",
    "1. First model was based on words posiotions with was outperformed by second model\n",
    "2. Second model was based on the repetition frequency of words for each class(aka. poet)<br><br>\n",
    "NOTE that both models are based on naive base decision process.\n",
    "<br><br>\n",
    "$$\n",
    "i^* = \\underset{i}{argmax} \\left\\{ f(w_i | \\underline{x})\\right\\}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "i^* = \\underset{i}{argmax} \\left\\{ \\frac{f(\\underline{x} | w_i) p(w_i)}{f(\\underline{x})}\\right\\} = \\underset{i}{argmax} \\left\\{f(\\underline{x} | w_i) p(w_i)\\right\\}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "i^* = \\underset{i}{argmax} \\left\\{\\underset{i}{\\prod}f({x_i} | w_i) p(w_i)\\right\\} = \n",
    "\\underset{i}{argmax} \\left\\{ p(w_i)\\underset{i}{\\prod}f({x_i} | w_i)\\right\\}\n",
    "$$\n",
    "<br><br>\n",
    "<h1>Discriminant Function</h1><br>\n",
    "*  Prior Probability $p(w_i)$ : The prior probability for a line of poem being belonged to each poet\n",
    "*  Posterior Probability $f(w_i | \\underline{x})$ : The posterior probability of a line of poem being belondged to each poet considering the evidence we have already seen\n",
    "*  Likelihood Function $f(\\underline{x} | w_i)$ : The distribution of poem lines for each poet\n",
    "*  Evidence $\\underline{x}$ : The data we have collected (assuming fair sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "Many factors could be used to evaluate a model, some which are illustrated blow<br><br>\n",
    "$$\n",
    "Recall = \\frac{\\text{Correct Detected Hafezez}}{\\text{All Hafezez}}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "Precision = \\frac{\\text{Correct Detected Hafezez}}{\\text{Detected Hafezez}}\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "Accuracy = \\frac{\\text{Correct Detected}}{\\text{ALL}}\n",
    "$$\n",
    "## Model I  : word based UNK soften\n",
    "$$\n",
    "Recall = 0.7688706846108836\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "Precision = 0.7291897891231964\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "Accuracy = 0.7886548587841072\n",
    "$$\n",
    "## Model II  : word based Laplace soft\n",
    "$$\n",
    "Recall = 0.7636044470450556\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "Precision = 0.742320819112628\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "Accuracy = 0.7948779320248923\n",
    "$$\n",
    "## Model III  : word based no soften\n",
    "$$\n",
    "Recall = 0.4739613809245173\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "Precision = 0.7612781954887218\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "Accuracy = 0.7240306366682623\n",
    "$$\n",
    "## Model IV : position based\n",
    "$$\n",
    "Recall = 0.4593329432416618\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "Precision = 0.5318428184281843\n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "Accuracy = 0.6134514121589277\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ca3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read train_test data and split them into two categries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'data.csv'\n",
    "f_train , f_test , l_train , l_test = take_data(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build models and train them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nlp_simple = Simple_NLP()\n",
    "nlp_cmplx = NLP_classifier()\n",
    "nlp_simple_0 = Simple_NLP(n = 0)\n",
    "nlp_laplace = Laplace_NLP(n = 0.37)\n",
    "\n",
    "nlp_simple.fit(copy.deepcopy(f_train) , l_train)\n",
    "nlp_cmplx.fit(copy.deepcopy(f_train) , l_train)\n",
    "nlp_simple_0.fit(copy.deepcopy(f_train) , l_train)\n",
    "nlp_laplace.fit(copy.deepcopy(f_train) , l_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_est_simple = nlp_simple.transform(copy.deepcopy(f_test))\n",
    "l_est_cmplx  = nlp_cmplx.transform(copy.deepcopy(f_test))\n",
    "l_est_simple_0 = nlp_simple_0.transform(copy.deepcopy(f_test))\n",
    "l_est_laplace = nlp_laplace.transform(copy.deepcopy(f_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_simple   = recall(l_est_simple , l_test)\n",
    "recall_cmplx    = recall(l_est_cmplx , l_test)\n",
    "recall_simple_0 = recall(l_est_simple_0 , l_test)\n",
    "recall_laplace  = recall(l_est_laplace , l_test)\n",
    "print(recall_simple , recall_cmplx , recall_simple_0 , recall_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec_simple     = precision(l_est_simple , l_test)\n",
    "prec_cmplx      = precision(l_est_cmplx , l_test)\n",
    "prec_simple_0   = precision(l_est_simple_0 , l_test)\n",
    "prec_laplace    = precision(l_est_laplace , l_test)\n",
    "print(prec_simple , prec_cmplx , prec_simple_0 , prec_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accurecy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_simple      = accuracy(l_est_simple , l_test)\n",
    "acc_cmplx       = accuracy(l_est_cmplx , l_test)\n",
    "acc_simple_0    = accuracy(l_est_simple_0 , l_test)\n",
    "acc_laplace     = accuracy(l_est_laplace , l_test)\n",
    "print(acc_simple , acc_cmplx , acc_simple_0 , acc_laplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Questions\n",
    "1.Precision : just take the following example , among all poem lines our model detects only on Hafez and it's detection is right. The corresponding precision whould be 1 but the over all performance of machine is not acceptable<br><br>\n",
    "2.Accuracy : just take the following example , 90 percent of poem lines are blonged to Hafez.In this case if our model always detects Hafez , the accuracy would be 0.9 but obvously the model lakes performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'evaluate.csv'\n",
    "valid = take_valid(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(valid)\n",
    "f_val = valid['text']\n",
    "i_val = valid['id']\n",
    "l_val = nlp_laplace.transform(copy.deepcopy(f_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.text = l_val\n",
    "valid.columns = ['id' , 'label']\n",
    "valid.to_csv('output.csv' , index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplace \n",
    "assume that word $x_j$ is only in class $w_1$ . lets compute $f(\\underline{x}| w_i)$ : <br>\n",
    "$f(\\underline{x} | w_i ) = \\underset{j}{\\prod}f(x_j | w_i)$<br>\n",
    "$p(x_j|w_1)\\neq 0 \\quad \\rightarrow \\quad \\underset{j}{\\prod}f(x_j | w_1) \\geq 0 \\quad \\rightarrow \\quad p(w_1|x) \\geq 0$ <br>\n",
    "$ p(x_j|w_2) = 0 \\quad \\rightarrow \\quad \\underset{j}{\\prod}f(x_j | w_2) = 0 \\quad \\rightarrow \\quad p(w_2|x) = 0$ <br>\n",
    "$w_1 = \\underset{i}{argmax}\\: p(w_i | x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution I\n",
    "A solution would be <b>Laplace smoothing</b> , which is a technique for smoothing categorical data. A small-sample correction, or pseudo-count, will be incorporated in every probability estimate. Consequently, <b>no probability will be zero</b>. this is a way of regularizing Naive Bayes, and when the pseudo-count is zero, it is called Laplace smoothing. While in the general case it is often called <b>Lidstone smoothing</b>.<br><br>\n",
    "In statistics, additive smoothing, also called Laplace smoothing (not to be confused with Laplacian smoothing), or Lidstone smoothing, is <b>a technique used to smooth categorical data</b>.<br><br>\n",
    "Given an observation x = (x1, …, xd) from a multinomial distribution with N trials and parameter vector θ = (θ1, …, θd), a \"smoothed\" version of the data gives the estimator:<br><br>\n",
    "$$\n",
    "\\hat{\\theta_i} = \\frac{x_i + \\alpha}{N + \\alpha d}\n",
    "$$\n",
    "<br><br>\n",
    "where the pseudocount α > 0 is the smoothing parameter (α = 0 corresponds to no smoothing). Additive smoothing is a type of shrinkage estimator, as the resulting estimate will be between the empirical estimate xi / N, and the uniform probability 1/d. Using Laplace's rule of succession, some authors have argued that α should be 1 (in which case the term add-one smoothing is also used), though in practice a smaller value is typically chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution II \n",
    "use and entry \"unknown\" (UNK in my model) to represent all uncommon word. using this approach would garentee non-zero probability for all words resulting in better performance "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
